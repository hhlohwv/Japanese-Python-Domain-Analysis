{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f817668c-4270-42d7-a69e-a6121f134aa7",
   "metadata": {},
   "source": [
    "# Japanese Similarity Analysis\n",
    "Auth: Harrison Loh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994a875-7d61-4f70-97e5-c134c0c2ec21",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaf9b9-41a5-4985-bc72-f6071223dd56",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Immersion learning is a method of foreign language learning (also called acquisition) which emphasizes the learning of a foreign language using native content in the language as the primary study material.\n",
    "For Japanese, one source of content for use in immersion learning is anime.\n",
    "Different methods and approaches for using anime to learn the Japanese language have been presented on different internet sites and platforms, one example being AJATT (All Japanese All The Time) [[1](https://tatsumoto-ren.github.io/blog/whats-ajatt.html)] and its various adaptations and modifications.\n",
    "\n",
    "One source of information relating to tips, strategies, and tools for applying an AJATT style approach to Japanese learning using Anime is a YouTube channel called Matt vs Japan [[2](https://www.youtube.com/@mattvsjapan)].\n",
    "One idea that has been presented by MattvsJapan, as well as on the Refold language learning guide is the idea of language \"domains\", or genres of content which have a specific subset of language that is commonly used (e.g. fantasy vs. crime drama vs. slice-of-life) [[3](https://refold.la/simplified/stage-2/b/immersion-guide)].\n",
    "By focusing on a single domain, words unique to a domain can be encountered more frequency, thus increasing the chance of acquiring them for long term retention.\n",
    "The aquisition of words has been deemed as highly important for learning a language, such as by Steve Kaufmann (one of the founders of LingQ) [[4](https://www.youtube.com/@Thelinguist)][[5](https://www.lingq.com/en/)].\n",
    "Therefore, focusing on a single domain when immersing is an attractive strategy for quickly aquiring foreign language vocabulary.\n",
    "\n",
    "One idea to determine the domain of a show/piece of content is by the genre of the media (e.g. slice-of-life).\n",
    "While this seems to be a sensible categorization of media into language domains, the question remains (at least to me) whether shows within a single genre quantitatively have a higher language similarity than shows across different tagged genres.\n",
    "\n",
    "The aim of this repo is to provide an analysis of the language content from different anime shows to quantify the degree of similarity in the language used.\n",
    "The objectives are as follows:\n",
    "- Develope criteria for comparing the similarity of the language present between any two shows.\n",
    "- Identify and differentiate between \"core language\" and \"domain language\".\n",
    "- Compare the degree of similarity of the language of shows in a single genre compared to shows across genres.\n",
    "\n",
    "References:\n",
    "- [1] https://tatsumoto-ren.github.io/blog/whats-ajatt.html\n",
    "- [2] https://www.youtube.com/@mattvsjapan\n",
    "- [3] https://refold.la/simplified/stage-2/b/immersion-guide\n",
    "- [4] https://www.youtube.com/@Thelinguist\n",
    "- [5] https://www.lingq.com/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c5f5f-1046-4ee6-af7c-52520b413b2c",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Dataset description\n",
    "The subtitles for 89 shows were obtained as the dataset for analysis.\n",
    "Subtitle files were downloaded from Kitsunekko.net under the Japanese subtitles page (https://kitsunekko.net/).\n",
    "For analysis, SRT subtitle files were solely used.\n",
    "For any shows with subtitles in the ASS format, conversion of these files to SRT was done using the subtitle tool Aegisub (https://aegisub.org/).\n",
    "The genres for the chosen shows were taken from the information present in their respective listings on MyAnimeList (https://myanimelist.net).\n",
    "A complete list of all the shows used with their genres and additional information is included in the \"show_genres.xlsx\" spreadsheet.\n",
    "The distribution of genres for the shows is as follow:\n",
    "\n",
    "- Action: 33\n",
    "- Drama: 26\n",
    "- Fantasy: 21\n",
    "- Sci-Fi: 18\n",
    "- Mystery: 17\n",
    "- Romance: 15\n",
    "- Adventure: 12\n",
    "- Comedy: 12\n",
    "- Sports: 11\n",
    "- Supernatural: 10\n",
    "- Slice of Life: 10\n",
    "- Suspense: 7\n",
    "- Ecchi: 2\n",
    "- Avant Garde: 1\n",
    "\n",
    "Given that the Ecchi and Avant Garde genre only show up a small number of times, these two categories are excluded from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma Extraction\n",
    "For quantifying the similarity between two or more selections of japanese text, the first step done is breaking the entire text into the component lemmas.\n",
    "In linguistics, lemmas are the \"dictionary form\" of a word, and can be thought of as the 'base' form.\n",
    "For example, in English the words _break_, _broke_, _broken_, and _breaking_ all share the same lemma: **break** (See [Wiki](https://en.wikipedia.org/wiki/Lemma_(morphology))).\n",
    "For a similarity analysis between two bodies of text, I am more interested in whether unique words are shared between shows, not whether the same forms of a word are shared.\n",
    "In other words, whether the base word 'to go' (行く) is shared, and not whether specific conjugations (such as 行きます, 行きません) are shared.\n",
    "Therefore, the lemmas present in a block of Japanese text are chosen as the components for further comparison.\n",
    "\n",
    "To give an example, consider the following 4 sentences, each with one additional change to the words used compared to the original, first sentence:\n",
    "- original: \"私の友達は親切な人です\"\n",
    "- one change: \"彼の友達は親切な人です\"\n",
    "- two changes: \"彼の彼女は親切な人です\"\n",
    "- three changes: \"彼の彼女は内気な人です\"\n",
    "\n",
    "With each sentence, the content becomes more distinct from the original sentence.\n",
    "\n",
    "Using the fugashi package with the Tagger class, we can extract the lemmas present in each of the above sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6754953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: 私の友達は親切な人です\n",
      "Original lemmas: ['私-代名詞', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "1 diff Sentence: 彼の友達は親切な人です\n",
      "1 diff lemmas: ['彼', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "2 diff Sentence: 彼の彼女は親切な人です\n",
      "2 diff lemmas: ['彼', 'の', '彼女', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "3 diff Sentence: 彼の彼女は内気な人です\n",
      "3 diff lemmas: ['彼', 'の', '彼女', 'は', '内気', 'だ', '人', 'です']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lemma extraction from text using fugashi\n",
    "\"\"\"\n",
    "from fugashi import Tagger\n",
    "\n",
    "def lemma_extract(text):\n",
    "    \"\"\"\n",
    "    Short function for returning a list of words and a list of the lemmas\n",
    "    \"\"\"\n",
    "    words = tagger(text)\n",
    "\n",
    "    lemma_list = []\n",
    "    for word in words:\n",
    "        lemma_list.append(word.feature.lemma)\n",
    "\n",
    "    return words, lemma_list\n",
    "\n",
    "\n",
    "tagger = Tagger('-Owakati')\n",
    "\n",
    "orig_sent = \"私の友達は親切な人です\"  # base sentence for comparison\n",
    "sent_1diff = \"彼の友達は親切な人です\"  # one word difference\n",
    "sent_2diff = \"彼の彼女は親切な人です\"  # two words different\n",
    "sent_3diff = \"彼の彼女は内気な人です\"  # three words different\n",
    "\n",
    "text = [orig_sent, sent_1diff, sent_2diff, sent_3diff]\n",
    "\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "for sentence in text:\n",
    "    words, lemmas = lemma_extract(sentence)\n",
    "\n",
    "    word_list.append(words)\n",
    "    lemma_list.append(lemmas)\n",
    "\n",
    "\n",
    "print(f\"Original Sentence: {text[0]}\")\n",
    "print(f\"Original lemmas: {lemma_list[0]}\\n\")\n",
    "print(f\"1 diff Sentence: {text[1]}\")\n",
    "print(f\"1 diff lemmas: {lemma_list[1]}\\n\")\n",
    "print(f\"2 diff Sentence: {text[2]}\")\n",
    "print(f\"2 diff lemmas: {lemma_list[2]}\\n\")\n",
    "print(f\"3 diff Sentence: {text[3]}\")\n",
    "print(f\"3 diff lemmas: {lemma_list[3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Similarity of Lemma sets\n",
    "\n",
    "For English text, similarity scores between sets of documents or text can be done fairly easily using criteria such as the Term Frequency-Inverse Document Frequency (IF-IDF) and libraries such as scikit with sklearn.\n",
    "One challenge I faced while trying to set these tools up however was in modifying the workflow from English to Japanese.\n",
    "While there are a few sites which describe adapting sklearn to Asian languages, specifically using the TfidfVectorizer class with a custom tokenizer (such as [here](https://investigate.ai/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/)) (which is what I originally wanted to do), I wasn't quite able to figure out how to apply this using the fugashi package which I was more comfortable with using, and so I decided to try a different approach.\n",
    "\n",
    "In the previous code block, lists of the lemmas present in each of the example sentences were generated.\n",
    "If the frequency of occurence of each lemma in the sentence is ignored, then each list can be converted into a set, resulting in a collection listing the unique lemmas present in a given text.\n",
    "From here, methods which quantify the similarity between two sets can be applied to quantify how similar the lemma collection between the sentences are.\n",
    "\n",
    "The value I am using to evaluate the similarity of sets is the [Jaccard Similarity Coefficient](https://en.wikipedia.org/wiki/Jaccard_index), and is defined as the size of the intersection between two sets divided by the size of the union of the sets.\n",
    "\n",
    "$$\n",
    "J(A, B) = \\frac{\\left| A \\cap B \\right|}{\\left| A \\cup B \\right|}\n",
    "$$\n",
    "\n",
    "where $A$ and $B$ are two sets for comparison.\n",
    "The calculation is commutative, so order of the sets does not matter.\n",
    "\n",
    "Setting up calculating the Jaccard Similarity can easily be done in python, as shown [here](https://www.annasguidetopython.com/python3/data%20structures/lists-finding-the-jaccard-similarity-between-two-sets-in-a-list/) and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 against 1: 1.0\n",
      "1 against 2: 0.7777777777777778\n",
      "1 against 3: 0.6\n",
      "1 against 4: 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quantifying the similarity of sentences using Jaccard Similarity on sets of the lemmas present\n",
    "\"\"\"\n",
    "set1 = set(lemma_list[0])\n",
    "set2 = set(lemma_list[1])\n",
    "set3 = set(lemma_list[2])\n",
    "set4 = set(lemma_list[3])\n",
    "\n",
    "def jaccard_similar(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "print(f\"1 against 1: {jaccard_similar(set1, set1)}\")\n",
    "print(f\"1 against 2: {jaccard_similar(set1, set2)}\")\n",
    "print(f\"1 against 3: {jaccard_similar(set1, set3)}\")\n",
    "print(f\"1 against 4: {jaccard_similar(set1, set4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the similarity value for a lemma set compared against itself is 1, meaning the sets are identical.\n",
    "As each sentence becomes more and more different than the original, the Jaccard coefficient decreases, with a value of ~0.45 for a sentence with 4 changed words from the original.\n",
    "\n",
    "To recap, in order to compare the similarity of the language used in between two anime shows, the following steps will be done:\n",
    "- extract a set of the lemmas present within the subtitle files of each show\n",
    "- calculate the Jaccard Similarity Coefficient between the shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "In this section, subtitle files for each show in the 'Data' folder will be parsed to create sets of the unique lemmas present, and the Jaccard Similarity between each show is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating database of lemmas from shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCreating a database of lemmas by reading in and parsing the subtitles files in the 'data' folder\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating a database of lemmas by reading in and parsing the subtitles files in the 'data' folder\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2de96-c8ba-4de9-a6fe-32231f1a7537",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357301ac-9f8c-438d-b9dc-db9551b7f6b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f643008-74ad-46b5-b682-8595fd5ffefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
