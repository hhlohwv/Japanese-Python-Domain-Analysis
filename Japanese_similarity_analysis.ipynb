{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f817668c-4270-42d7-a69e-a6121f134aa7",
   "metadata": {},
   "source": [
    "# Japanese Similarity Analysis\n",
    "Auth: Harrison Loh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994a875-7d61-4f70-97e5-c134c0c2ec21",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaf9b9-41a5-4985-bc72-f6071223dd56",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Immersion learning is a method of foreign language learning (also called acquisition) which emphasizes the learning of a foreign language using native content in the language as the primary study material.\n",
    "For Japanese, one source of content for use in immersion learning is anime.\n",
    "Different methods and approaches for using anime to learn the Japanese language have been presented on different internet sites and platforms, one example being AJATT (All Japanese All The Time) [[1](https://tatsumoto-ren.github.io/blog/whats-ajatt.html)] and its various adaptations and modifications.\n",
    "\n",
    "One source of information relating to tips, strategies, and tools for applying an AJATT style approach to Japanese learning using Anime is a YouTube channel called Matt vs Japan [[2](https://www.youtube.com/@mattvsjapan)].\n",
    "One idea that has been presented by MattvsJapan, as well as on the Refold language learning guide is the idea of language \"domains\", or genres of content which have a specific subset of language that is commonly used (e.g. fantasy vs. crime drama vs. slice-of-life) [[3](https://refold.la/simplified/stage-2/b/immersion-guide)].\n",
    "By focusing on a single domain, words unique to a domain can be encountered more frequency, thus increasing the chance of acquiring them for long term retention.\n",
    "The aquisition of words has been deemed as highly important for learning a language, such as by Steve Kaufmann (one of the founders of LingQ) [[4](https://www.youtube.com/@Thelinguist)][[5](https://www.lingq.com/en/)].\n",
    "Therefore, focusing on a single domain when immersing is an attractive strategy for quickly aquiring foreign language vocabulary.\n",
    "\n",
    "One idea to determine the domain of a show/piece of content is by the genre of the media (e.g. slice-of-life).\n",
    "While this seems to be a sensible categorization of media into language domains, the question remains (at least to me) whether shows within a single genre quantitatively have a higher language similarity than shows across different tagged genres.\n",
    "\n",
    "The aim of this repo is to provide an analysis of the language content from different anime shows to quantify the degree of similarity in the language used.\n",
    "The objectives are as follows:\n",
    "- Develope criteria for comparing the similarity of the language present between any two shows.\n",
    "- Identify and differentiate between \"core language\" and \"domain language\".\n",
    "- Compare the degree of similarity of the language of shows in a single genre compared to shows across genres.\n",
    "\n",
    "References:\n",
    "- [1] https://tatsumoto-ren.github.io/blog/whats-ajatt.html\n",
    "- [2] https://www.youtube.com/@mattvsjapan\n",
    "- [3] https://refold.la/simplified/stage-2/b/immersion-guide\n",
    "- [4] https://www.youtube.com/@Thelinguist\n",
    "- [5] https://www.lingq.com/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c5f5f-1046-4ee6-af7c-52520b413b2c",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Dataset description\n",
    "The subtitles for 89 shows were obtained as the dataset for analysis.\n",
    "Subtitle files were downloaded from Kitsunekko.net under the Japanese subtitles page (https://kitsunekko.net/).\n",
    "For analysis, SRT subtitle files were solely used.\n",
    "For any shows with subtitles in the ASS format, conversion of these files to SRT was done using the subtitle tool Aegisub (https://aegisub.org/).\n",
    "The genres for the chosen shows were taken from the information present in their respective listings on MyAnimeList (https://myanimelist.net).\n",
    "A complete list of all the shows used with their genres and additional information is included in the \"show_genres.xlsx\" spreadsheet.\n",
    "The distribution of genres for the shows is as follow:\n",
    "\n",
    "- Action: 33\n",
    "- Drama: 26\n",
    "- Fantasy: 21\n",
    "- Sci-Fi: 18\n",
    "- Mystery: 17\n",
    "- Romance: 15\n",
    "- Adventure: 12\n",
    "- Comedy: 12\n",
    "- Sports: 11\n",
    "- Supernatural: 10\n",
    "- Slice of Life: 10\n",
    "- Suspense: 7\n",
    "- Ecchi: 2\n",
    "- Avant Garde: 1\n",
    "\n",
    "Given that the Ecchi and Avant Garde genre only show up a small number of times, these two categories are excluded from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma Extraction\n",
    "For quantifying the similarity between two or more selections of japanese text, the first step done is breaking the entire text into the component lemmas.\n",
    "In linguistics, lemmas are the \"dictionary form\" of a word, and can be thought of as the 'base' form.\n",
    "For example, in English the words _break_, _broke_, _broken_, and _breaking_ all share the same lemma: **break** (See [Wiki](https://en.wikipedia.org/wiki/Lemma_(morphology))).\n",
    "For a similarity analysis between two bodies of text, I am more interested in whether unique words are shared between shows, not whether the same forms of a word are shared.\n",
    "In other words, whether the base word 'to go' (行く) is shared, and not whether specific conjugations (such as 行きます, 行きません) are shared.\n",
    "Therefore, the lemmas present in a block of Japanese text are chosen as the components for further comparison.\n",
    "\n",
    "To give an example, consider the following 4 sentences, each with one additional change to the words used compared to the original, first sentence:\n",
    "- original: \"私の友達は親切な人です\"\n",
    "- one change: \"彼の友達は親切な人です\"\n",
    "- two changes: \"彼の彼女は親切な人です\"\n",
    "- three changes: \"彼の彼女は内気な人です\"\n",
    "\n",
    "With each sentence, the content becomes more distinct from the original sentence.\n",
    "\n",
    "Using the fugashi package with the Tagger class, we can extract the lemmas present in each of the above sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6754953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 私の友達は親切な人です\n",
      "Words: [私, の, 友達, は, 親切, な, 人, です]\n",
      "Lemmas: ['私-代名詞', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の友達は親切な人です\n",
      "Words: [彼, の, 友達, は, 親切, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の彼女は親切な人です\n",
      "Words: [彼, の, 彼女, は, 親切, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '彼女', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の彼女は内気な人です\n",
      "Words: [彼, の, 彼女, は, 内気, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '彼女', 'は', '内気', 'だ', '人', 'です']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugashi import Tagger\n",
    "\n",
    "def lemma_extract(text):\n",
    "    \"\"\"\n",
    "    Short function for returning a list of words and a list of the lemmas\n",
    "    \"\"\"\n",
    "    words = tagger(text)\n",
    "\n",
    "    lemma_list = []\n",
    "    for word in words:\n",
    "        lemma_list.append(word.feature.lemma)\n",
    "\n",
    "    return words, lemma_list\n",
    "\n",
    "\n",
    "tagger = Tagger('-Owakati')\n",
    "\n",
    "orig_sent = \"私の友達は親切な人です\"  # base sentence for comparison\n",
    "sent_1diff = \"彼の友達は親切な人です\"  # one word difference\n",
    "sent_2diff = \"彼の彼女は親切な人です\"  # two words different\n",
    "sent_3diff = \"彼の彼女は内気な人です\"  # three words different\n",
    "\n",
    "text = [orig_sent, sent_1diff, sent_2diff, sent_3diff]\n",
    "\n",
    "for sentence in text:\n",
    "    words, lemma_list = lemma_extract(sentence)\n",
    "\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Words: {words}\")\n",
    "    print(f\"Lemmas: {lemma_list}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Similarity of Lemma sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Y/12/06 12:16:18]WARNING - text_preprocess.py#<module>:29: neologdn package is not installed yet. You could not call neologd dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Y/12/06 12:16:18]WARNING - mecab_wrapper.py#<module>:28: neologdn package is not installed yet. You could not call neologd dictionary.\n",
      "[Y/12/06 12:16:19]WARNING - kytea_wrapper.py#<module>:19: Mykytea is not ready to use yet. Install first if you would like to use kytea wrapper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 私の友達は親切な人です\n",
      "Words: [私, の, 友達, は, 親切, な, 人, です]\n",
      "Lemmas: ['私-代名詞', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の友達は親切な人です\n",
      "Words: [彼, の, 友達, は, 親切, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の彼女は親切な人です\n",
      "Words: [彼, の, 彼女, は, 親切, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '彼女', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "Sentence: 彼の彼女は内気な人です\n",
      "Words: [彼, の, 彼女, は, 内気, な, 人, です]\n",
      "Lemmas: ['彼', 'の', '彼女', 'は', '内気', 'だ', '人', 'です']\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'tokenizer' parameter of TfidfVectorizer must be a callable or None. Got <module 'JapaneseTokenizer' from 'e:\\\\Users\\\\Harrison\\\\Documents\\\\GitHub\\\\Japanese-Similarity-Analysis\\\\.venv\\\\Lib\\\\site-packages\\\\JapaneseTokenizer\\\\__init__.py'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32me:\\Users\\Harrison\\Documents\\GitHub\\Japanese-Similarity-Analysis\\Japanese_similarity_analysis.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Users/Harrison/Documents/GitHub/Japanese-Similarity-Analysis/Japanese_similarity_analysis.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Vectorizer and count words (with a custom tokenizer)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Users/Harrison/Documents/GitHub/Japanese-Similarity-Analysis/Japanese_similarity_analysis.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(tokenizer\u001b[39m=\u001b[39mJapaneseTokenizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Users/Harrison/Documents/GitHub/Japanese-Similarity-Analysis/Japanese_similarity_analysis.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m test \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(lemma_total)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Users/Harrison/Documents/GitHub/Japanese-Similarity-Analysis/Japanese_similarity_analysis.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(test)\n",
      "File \u001b[1;32me:\\Users\\Harrison\\Documents\\GitHub\\Japanese-Similarity-Analysis\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\Harrison\\Documents\\GitHub\\Japanese-Similarity-Analysis\\.venv\\Lib\\site-packages\\sklearn\\base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[0;32m   1141\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1145\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Users\\Harrison\\Documents\\GitHub\\Japanese-Similarity-Analysis\\.venv\\Lib\\site-packages\\sklearn\\base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    640\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    641\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    642\u001b[0m     )\n",
      "File \u001b[1;32me:\\Users\\Harrison\\Documents\\GitHub\\Japanese-Similarity-Analysis\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'tokenizer' parameter of TfidfVectorizer must be a callable or None. Got <module 'JapaneseTokenizer' from 'e:\\\\Users\\\\Harrison\\\\Documents\\\\GitHub\\\\Japanese-Similarity-Analysis\\\\.venv\\\\Lib\\\\site-packages\\\\JapaneseTokenizer\\\\__init__.py'> instead."
     ]
    }
   ],
   "source": [
    "from fugashi import Tagger\n",
    "\n",
    "def lemma_extract(text):\n",
    "    \"\"\"\n",
    "    Short function for returning a list of words and a list of the lemmas\n",
    "    \"\"\"\n",
    "    words = tagger(text)\n",
    "\n",
    "    lemma_list = []\n",
    "    for word in words:\n",
    "        lemma_list.append(word.feature.lemma)\n",
    "\n",
    "    return words, lemma_list\n",
    "\n",
    "\n",
    "tagger = Tagger('-Owakati')\n",
    "\n",
    "orig_sent = \"私の友達は親切な人です\"  # base sentence for comparison\n",
    "sent_1diff = \"彼の友達は親切な人です\"  # one word difference\n",
    "sent_2diff = \"彼の彼女は親切な人です\"  # two words different\n",
    "sent_3diff = \"彼の彼女は内気な人です\"  # three words different\n",
    "\n",
    "text = [orig_sent, sent_1diff, sent_2diff, sent_3diff]\n",
    "\n",
    "lemma_total = []\n",
    "for sentence in text:\n",
    "    words, lemma_list = lemma_extract(sentence)\n",
    "\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Words: {words}\")\n",
    "    print(f\"Lemmas: {lemma_list}\\n\")\n",
    "\n",
    "    lemma_total.append(lemma_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2de96-c8ba-4de9-a6fe-32231f1a7537",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357301ac-9f8c-438d-b9dc-db9551b7f6b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f643008-74ad-46b5-b682-8595fd5ffefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
