{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f817668c-4270-42d7-a69e-a6121f134aa7",
   "metadata": {},
   "source": [
    "# Japanese Similarity Analysis\n",
    "Auth: Harrison Loh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994a875-7d61-4f70-97e5-c134c0c2ec21",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaf9b9-41a5-4985-bc72-f6071223dd56",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Immersion learning is a method of foreign language learning (also called acquisition) which emphasizes the learning of a foreign language using native content in the language as the primary study material.\n",
    "For Japanese, one source of content for use in immersion learning is anime.\n",
    "Different methods and approaches for using anime to learn the Japanese language have been presented on different internet sites and platforms, one example being AJATT (All Japanese All The Time) [[1](https://tatsumoto-ren.github.io/blog/whats-ajatt.html)] and its various adaptations and modifications.\n",
    "\n",
    "One source of information relating to tips, strategies, and tools for applying an AJATT style approach to Japanese learning using Anime is a YouTube channel called Matt vs Japan [[2](https://www.youtube.com/@mattvsjapan)].\n",
    "One idea that has been presented by MattvsJapan, as well as on the Refold language learning guide is the idea of language \"domains\", or genres of content which have a specific subset of language that is commonly used (e.g. fantasy vs. crime drama vs. slice-of-life) [[3](https://refold.la/simplified/stage-2/b/immersion-guide)].\n",
    "By focusing on a single domain, words unique to a domain can be encountered more frequency, thus increasing the chance of acquiring them for long term retention.\n",
    "The aquisition of words has been deemed as highly important for learning a language, such as by Steve Kaufmann (one of the founders of LingQ) [[4](https://www.youtube.com/@Thelinguist)][[5](https://www.lingq.com/en/)].\n",
    "Therefore, focusing on a single domain when immersing is an attractive strategy for quickly aquiring foreign language vocabulary.\n",
    "\n",
    "One idea to determine the domain of a show/piece of content is by the genre of the media (e.g. slice-of-life).\n",
    "While this seems to be a sensible categorization of media into language domains, the question remains (at least to me) whether shows within a single genre quantitatively have a higher language similarity than shows across different tagged genres.\n",
    "\n",
    "The aim of this repo is to provide an analysis of the language content from different anime shows to quantify the degree of similarity in the language used.\n",
    "The objectives are as follows:\n",
    "- Develope criteria for comparing the similarity of the language present between any two shows.\n",
    "- Identify and differentiate between \"core language\" and \"domain language\".\n",
    "- Compare the degree of similarity of the language of shows in a single genre compared to shows across genres.\n",
    "\n",
    "References:\n",
    "- [1] https://tatsumoto-ren.github.io/blog/whats-ajatt.html\n",
    "- [2] https://www.youtube.com/@mattvsjapan\n",
    "- [3] https://refold.la/simplified/stage-2/b/immersion-guide\n",
    "- [4] https://www.youtube.com/@Thelinguist\n",
    "- [5] https://www.lingq.com/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c5f5f-1046-4ee6-af7c-52520b413b2c",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Dataset description\n",
    "The subtitles for 89 shows were obtained as the dataset for analysis.\n",
    "Subtitle files were downloaded from Kitsunekko.net under the Japanese subtitles page (https://kitsunekko.net/).\n",
    "For analysis, SRT subtitle files were solely used.\n",
    "For any shows with subtitles in the ASS format, conversion of these files to SRT was done using the subtitle tool Aegisub (https://aegisub.org/) by exporting as SRT files after choosing the \"clean tags\" option in the export window.\n",
    "The genres for the chosen shows were taken from the information present in their respective listings on MyAnimeList (https://myanimelist.net).\n",
    "A complete list of all the shows used with their genres and additional information is included in the \"show_genres.xlsx\" spreadsheet.\n",
    "The distribution of genres for the shows is as follow:\n",
    "\n",
    "- Action: 33\n",
    "- Drama: 26\n",
    "- Fantasy: 21\n",
    "- Sci-Fi: 18\n",
    "- Mystery: 17\n",
    "- Romance: 15\n",
    "- Adventure: 12\n",
    "- Comedy: 12\n",
    "- Sports: 11\n",
    "- Supernatural: 10\n",
    "- Slice of Life: 10\n",
    "- Suspense: 7\n",
    "- Ecchi: 2\n",
    "- Avant Garde: 1\n",
    "\n",
    "Given that the Ecchi and Avant Garde genre only show up a small number of times, these two categories are excluded from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma Extraction\n",
    "For quantifying the similarity between two or more selections of japanese text, the first step done is breaking the entire text into the component lemmas.\n",
    "In linguistics, lemmas are the \"dictionary form\" of a word, and can be thought of as the 'base' form.\n",
    "For example, in English the words _break_, _broke_, _broken_, and _breaking_ all share the same lemma: **break** (See [Wiki](https://en.wikipedia.org/wiki/Lemma_(morphology))).\n",
    "For a similarity analysis between two bodies of text, I am more interested in whether unique words are shared between shows, not whether the same forms of a word are shared.\n",
    "In other words, whether the base word 'to go' (行く) is shared, and not whether specific conjugations (such as 行きます, 行きません) are shared.\n",
    "Therefore, the lemmas present in a block of Japanese text are chosen as the components for further comparison.\n",
    "\n",
    "To give an example, consider the following 4 sentences, each with one additional change to the words used compared to the original, first sentence:\n",
    "- original: \"私の友達は親切な人です\"\n",
    "- one change: \"彼の友達は親切な人です\"\n",
    "- two changes: \"彼の彼女は親切な人です\"\n",
    "- three changes: \"彼の彼女は内気な人です\"\n",
    "\n",
    "With each sentence, the content becomes more distinct from the original sentence.\n",
    "\n",
    "Using the fugashi package with the Tagger class, we can extract the lemmas present in each of the above sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6754953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: 私の友達は親切な人です\n",
      "Original lemmas: ['私-代名詞', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "1 diff Sentence: 彼の友達は親切な人です\n",
      "1 diff lemmas: ['彼', 'の', '友達', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "2 diff Sentence: 彼の彼女は親切な人です\n",
      "2 diff lemmas: ['彼', 'の', '彼女', 'は', '親切', 'だ', '人', 'です']\n",
      "\n",
      "3 diff Sentence: 彼の彼女は内気な人です\n",
      "3 diff lemmas: ['彼', 'の', '彼女', 'は', '内気', 'だ', '人', 'です']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lemma extraction from text using fugashi\n",
    "\"\"\"\n",
    "from fugashi import Tagger\n",
    "\n",
    "def lemma_extract(text):\n",
    "    \"\"\"\n",
    "    Short function for returning a list of words and a list of the lemmas\n",
    "    \"\"\"\n",
    "    words = tagger(text)\n",
    "\n",
    "    lemma_list = []\n",
    "    for word in words:\n",
    "        lemma_list.append(word.feature.lemma)\n",
    "\n",
    "    return words, lemma_list\n",
    "\n",
    "\n",
    "tagger = Tagger('-Owakati')\n",
    "\n",
    "orig_sent = \"私の友達は親切な人です\"  # base sentence for comparison\n",
    "sent_1diff = \"彼の友達は親切な人です\"  # one word difference\n",
    "sent_2diff = \"彼の彼女は親切な人です\"  # two words different\n",
    "sent_3diff = \"彼の彼女は内気な人です\"  # three words different\n",
    "\n",
    "text = [orig_sent, sent_1diff, sent_2diff, sent_3diff]\n",
    "\n",
    "word_list = []\n",
    "lemma_list = []\n",
    "for sentence in text:\n",
    "    words, lemmas = lemma_extract(sentence)\n",
    "\n",
    "    word_list.append(words)\n",
    "    lemma_list.append(lemmas)\n",
    "\n",
    "\n",
    "print(f\"Original Sentence: {text[0]}\")\n",
    "print(f\"Original lemmas: {lemma_list[0]}\\n\")\n",
    "print(f\"1 diff Sentence: {text[1]}\")\n",
    "print(f\"1 diff lemmas: {lemma_list[1]}\\n\")\n",
    "print(f\"2 diff Sentence: {text[2]}\")\n",
    "print(f\"2 diff lemmas: {lemma_list[2]}\\n\")\n",
    "print(f\"3 diff Sentence: {text[3]}\")\n",
    "print(f\"3 diff lemmas: {lemma_list[3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Similarity of Lemma sets\n",
    "\n",
    "For English text, similarity scores between sets of documents or text can be done fairly easily using criteria such as the Term Frequency-Inverse Document Frequency (IF-IDF) and libraries such as scikit with sklearn.\n",
    "One challenge I faced while trying to set these tools up however was in modifying the workflow from English to Japanese.\n",
    "While there are a few sites which describe adapting sklearn to Asian languages, specifically using the TfidfVectorizer class with a custom tokenizer (such as [here](https://investigate.ai/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/)) (which is what I originally wanted to do), I wasn't quite able to figure out how to apply this using the fugashi package which I was more comfortable with using, and so I decided to try a different approach.\n",
    "\n",
    "In the previous code block, lists of the lemmas present in each of the example sentences were generated.\n",
    "If the frequency of occurence of each lemma in the sentence is ignored, then each list can be converted into a set, resulting in a collection listing the unique lemmas present in a given text.\n",
    "From here, methods which quantify the similarity between two sets can be applied to quantify how similar the lemma collection between the sentences are.\n",
    "\n",
    "The value I am using to evaluate the similarity of sets is the [Jaccard Similarity Coefficient](https://en.wikipedia.org/wiki/Jaccard_index), and is defined as the size of the intersection between two sets divided by the size of the union of the sets.\n",
    "\n",
    "$$\n",
    "J(A, B) = \\frac{\\left| A \\cap B \\right|}{\\left| A \\cup B \\right|}\n",
    "$$\n",
    "\n",
    "where $A$ and $B$ are two sets for comparison.\n",
    "The calculation is commutative, so order of the sets does not matter.\n",
    "\n",
    "Setting up calculating the Jaccard Similarity can easily be done in python, as shown [here](https://www.annasguidetopython.com/python3/data%20structures/lists-finding-the-jaccard-similarity-between-two-sets-in-a-list/) and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 against 1: 1.0\n",
      "1 against 2: 0.7777777777777778\n",
      "1 against 3: 0.6\n",
      "1 against 4: 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quantifying the similarity of sentences using Jaccard Similarity on sets of the lemmas present\n",
    "\"\"\"\n",
    "set1 = set(lemma_list[0])\n",
    "set2 = set(lemma_list[1])\n",
    "set3 = set(lemma_list[2])\n",
    "set4 = set(lemma_list[3])\n",
    "\n",
    "def jaccard_similar(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "print(f\"1 against 1: {jaccard_similar(set1, set1)}\")\n",
    "print(f\"1 against 2: {jaccard_similar(set1, set2)}\")\n",
    "print(f\"1 against 3: {jaccard_similar(set1, set3)}\")\n",
    "print(f\"1 against 4: {jaccard_similar(set1, set4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the similarity value for a lemma set compared against itself is 1, meaning the sets are identical.\n",
    "As each sentence becomes more and more different than the original, the Jaccard coefficient decreases, with a value of ~0.45 for a sentence with 4 changed words from the original.\n",
    "\n",
    "To recap, in order to compare the similarity of the language used in between two anime shows, the following steps will be done:\n",
    "- extract a set of the lemmas present within the subtitle files of each show\n",
    "- calculate the Jaccard Similarity Coefficient between the shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "In this section, subtitle files for each show in the 'Data' folder will be parsed to create sets of the unique lemmas present, and the Jaccard Similarity between each show is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating database of lemmas from shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Beginning parse of shows in subtitle folder --\n",
      "\n",
      "Show currently parsing: 07-ghost\n",
      "Show currently parsing: 3-gatsu-no-lion\n",
      "Show currently parsing: 7seeds\n",
      "Show currently parsing: 91-days\n",
      "Show currently parsing: acca-13\n",
      "Show currently parsing: aico-incarnation\n",
      "Show currently parsing: akebi-chan-no-sailor-fuku\n",
      "Show currently parsing: amagi-brilliant-park\n",
      "Show currently parsing: appare-ranman\n",
      "Show currently parsing: assassination-classroom\n",
      "Show currently parsing: baby-steps\n",
      "Show currently parsing: ballroom-e-youkoso\n",
      "Show currently parsing: banana-fish\n",
      "Show currently parsing: barakamon\n",
      "Show currently parsing: blue-lock\n",
      "Show currently parsing: blue-period\n",
      "Show currently parsing: bocchi-the-rock\n",
      "Show currently parsing: boku-no-hero-academia\n",
      "Show currently parsing: bungo-stray-dogs\n",
      "Show currently parsing: burn-the-witch\n",
      "Show currently parsing: chainsaw-man\n",
      "Show currently parsing: charlotte\n",
      "Show currently parsing: chihayafuru\n",
      "Show currently parsing: cider-no-you-ni-kotoba-ga-wakiagaru\n",
      "Show currently parsing: death-note\n",
      "Show currently parsing: deca-dence\n",
      "Show currently parsing: dr-stone\n",
      "Show currently parsing: drifting-dragons\n",
      "Show currently parsing: eighty-six\n",
      "Show currently parsing: fire-force\n",
      "Show currently parsing: flying-witch\n",
      "Show currently parsing: free\n",
      "Show currently parsing: fruits-basket\n",
      "Show currently parsing: fugou-keiji\n",
      "Show currently parsing: giant-killing\n",
      "Show currently parsing: gleipnir\n",
      "Show currently parsing: great-pretender\n",
      "Show currently parsing: haikyuu\n",
      "Show currently parsing: hibike-euphonium\n",
      "Show currently parsing: horimiya\n",
      "Show currently parsing: hyouka\n",
      "Show currently parsing: id-invaded\n",
      "Show currently parsing: iron-blooded-orphans\n",
      "Show currently parsing: jujutsu-kaisen\n",
      "Show currently parsing: k-return-of-kings\n",
      "Show currently parsing: kabukichou-sherlock\n",
      "Show currently parsing: kaze-ga-tsuyoku-fuiteiru\n",
      "Show currently parsing: kobayashi-san-dragon-maid\n",
      "Show currently parsing: kuroko-no-basket\n",
      "Show currently parsing: lycoris-recoil\n",
      "Show currently parsing: magic-kaito-1412\n",
      "Show currently parsing: mahoutsukai-no-yome\n",
      "Show currently parsing: maou-gakuin-no-futekigousha\n",
      "Show currently parsing: mawaru-penguindrum\n",
      "Show currently parsing: megalobox\n",
      "Show currently parsing: mushishi\n",
      "Show currently parsing: nagi-no-asukara\n",
      "Show currently parsing: no-guns-life\n",
      "Show currently parsing: orbital-children\n",
      "Show currently parsing: paripi-koumei\n",
      "Show currently parsing: peachboy-riverside\n",
      "Show currently parsing: planetes\n",
      "Show currently parsing: plunderer\n",
      "Show currently parsing: princess-principal\n",
      "Show currently parsing: psycho-pass\n",
      "Show currently parsing: relife\n",
      "Show currently parsing: sayounara-zetsubou-sensei\n",
      "Show currently parsing: shirobako\n",
      "Show currently parsing: sirius-the-jaeger\n",
      "Show currently parsing: ssss-dynazenon\n",
      "Show currently parsing: standing-on-a-million-lives\n",
      "Show currently parsing: suisei-no-majo\n",
      "Show currently parsing: sukitte-ii-na-yo\n",
      "Show currently parsing: summer-time-rendering\n",
      "Show currently parsing: takt-op-destiny\n",
      "Show currently parsing: tales-of-zestiria-the-x\n",
      "Show currently parsing: tamako-love-story\n",
      "Show currently parsing: tanaka-kun-wa-itsumo-kedaruge\n",
      "Show currently parsing: tantei-wa-mou-shindeiru\n",
      "Show currently parsing: tonari-no-kaibutsu-kun\n",
      "Show currently parsing: usagi-drop\n",
      "Show currently parsing: vanitas-no-karte\n",
      "Show currently parsing: vinland-saga\n",
      "Show currently parsing: violet-evergarden\n",
      "Show currently parsing: vivy-flourite-eyes-song\n",
      "Show currently parsing: wotaku-ni-koi-wa-muzukashii\n",
      "Show currently parsing: yakusoku-no-neverland\n",
      "Show currently parsing: yowamushi-pedal\n",
      "Show currently parsing: yuru-camp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating a database of lemmas by reading in and parsing the subtitles files in the 'data' folder\n",
    "\"\"\"\n",
    "from fugashi import Tagger\n",
    "from subtitleparsing import create_lemma_database\n",
    "\n",
    "data_folder = 'data'  # folder with subtitles\n",
    "tagger = Tagger('-Owakati')\n",
    "\n",
    "lemma_database = create_lemma_database(data_folder, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9344c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'シチュー-stew', '只', '処分', '無い', '盛り上がる', 'えー', '男', '可能', '近付く', '追い追い', '彼', '信ずる', '好み', '一塁', '年', '熟す', '襲う', '終わり', 'ピッチャー-pitcher', '知れる', '病み上がり', '布団', '体調', '日本', 'やんちゃ', '小さい', '次', '倒す', '未だ', '馬鹿馬鹿しい', '強いる', '体', '禁止', '見付かる', '忘れ物', '両方', '仲間', '無量', '巻き返す', '絶景', '親元', '女の子', 'パスタ-pasta', '匙', '糞', '委員', '其の', '直接', 'じゃん', '此処', 'のみ', '宜しく', '一寸', '放課', '暴力', '追う', '雨霰', '普通', '何方', 'ワン-one', '彗星', '頭脳', '自身', '静か', '着る', '会', '下される', '寝込む', '勝つ', '御早う', '嘗て', '五', '局', '見舞い', '代走', '合宿', 'どんな', '性格', '他人', '都合', '失敗', '連行', '初めて', '見え透く', '選手', 'コニシ', '台', 'オーライ-all right', '如何に', '業者', '変わり', '幾', '聞く', '勿論', '読者', '安心', '単刀', '騙す', '安全', '小学', '仕方無い', '派', '一員', 'けれど', '地', 'あからさま', '付ける', 'ばき', '我慢', '急行', '的', '利用', 'フライング-flying', '平凡', 'ない', '番', '悪', '料理', '多重', '今日は', '選び取る', '新', '空', '所', '首尾', 'ジャミング-jamming', '死人', '影', '見失う', '無し', '品', '仕方', '未', '訳', 'でかい', '揺らす', 'マンション-mansion', '好', '連れ出す', '親友', '払う', '運動', '唯一', '観念', '得る', '空く', '折る', '収穫', '驕る', '演ずる', '私欲', '劇', 'タカギ', 'イチロウ', '男性', '降板', 'プロテクター-protector', '済む', '場', '果て', '縦し', 'こう', '下着', '怪我', 'テスト-test', '品定め', '僕-代名詞', '感', '最低', '環境', '久々', '弓', 'セキ', '伺う', '商', '日', 'べし', 'ポスト-post', '傷', '書', 'カッター-cutter', '仮', '自体', '行動', '血', '登校', '終了', '翌日', 'テント-tent', '暫く', 'ずぶ濡れ', 'カット-cut', '伸ばす', '悉く', '恥ずかしい', '居着く', '紙袋', '規則', '平熱', 'カツ-cutlet', '西', '件', '听', '動画', '貸す', '包む', 'びっくり', '突', '降霊', '興味', 'スピード-speed', '予感', '立つ', '是非', 'くらい', 'リサーチ-research', 'スペシャル-special', '羽根', 'ぜ', '任務', '込む', '残酷', 'アイドル-idol', 'その', '我が', '回', '前髪', '朝食', '隠れる', 'ラーメン-Rahmen', '徹底', '休める', '着席', '哲学', '新しい', '打撲', '願う', '又', '偶然', '下座', '其々', '眠る', '発見', '案内', '感謝', '自己', '如何', '従う', '信頼', '期', '当然', '余り', '憑依', 'わ', '興奮', 'アイディア-idea', '形', 'バイト-Arbeit', '代表', '山', '日課', '内申', '容疑', '御陰', '一番', '現場', '思い込む', '歳', 'ニュー-new', '滑茸', 'しも', '盛大', '軈て', 'ハイ-high', '温かい', '続ける', '存在', '此の世', '唐突', '切る', 'プロ-pro', '記事', '起こす', 'より', '甲子', 'てまえ', '前', '蛸', 'トラック-truck', '初めまして', '彼れ', '崩れる', '漏らす', '大物', '言い草', '海', '服', '限界', '当てる', '満点', '悟る', 'キャッチャー-catcher', '作り物', '黒', 'ちゃう', '名高い', '悲しむ', '抜け駆け', '優先', '見下ろす', 'サボる', '然し', '野球', '野菜', 'ず', '戻る', '学年', '号', '殆ど', '入院', '邪魔', '怖い', '握り', '間違える', '器用', '振り返す', '至急', '諦める', '責任', '着', '混ざる', 'にて', '家族', '誘う', '受ける', 'ビル-building', 'まさか', '察', '違い', '八百', 'まい', '帰省', '事故', '繰り返す', '偏る', '用', '思う', '沢山', '語る', 'ニュース-news', '洗う', 'っち', '頂く', '徹する', '等', '兄弟', '告白', '焼き立て', '音色', '内容', '偉い', '転落', '発言', '武道', '下る', '本当', '手伝い', '向こう', '燃やす', '能', '傾向', '許可', '交際', 'トラップ-trap', 'クライ-cry', 'いんちき', '忙しい', '手', '頼む', '彼氏', '無理', 'て', '糸', '成る', '断ち切る', 'リーグ-league', '他', '連れる', '原付き', '全球', '闘病', '馬鹿げる', '野放し', '実験', 'セット-set', '怒る', '中辛', '求める', '触れる', '答え', 'ながら', '今度', '見せ所', '真似る', '歌', '確り', '指', '彼方', 'チーム-team', '高熱', '優しい', '日差し', 'イエス-Iesous', 'ぽい', '収まる', 'こんな', '雲', '置く', '見せびらかす', '着替える', '垂れ', '詰め込む', '作る', '妹', '本気', '口癖', '約', '易い', 'ホーム-home', 'システム-system', '真っ当', '井戸', '兎に角', '続く', '目覚める', '次第', '人事', '不味い', '表わす', '晩御飯', '踊る', '方', 'プレー-play', '只今', 'そう', 'ティーン-teen', '存知', 'さん', 'ニシモリ', '流れ', '思い付く', '都内', '会う', 'コンビニ-convenience', '幸せ', '欲しい', 'ビザ-visa', '確保', '本題', '本', '時代', 'さえ', '鍛える', 'ウ', '体育', '双', '教頭', '呼ぶ', '甚だ', '母', '大きい', '注目', '可愛い', '買う', '」', '少年', '悪魔', '取る', '到着', None, 'ＷＷＷ', '漸と', '開始', 'メリット-merit', '自然', '休み', '昼', '近付き', '嘘', '雪', '悪用', '反応', '離れる', '無償', 'むかつく', 'タイヨウ', 'ＣＭ', '一向', '直後', '呼び出し', '制', '部長', '柚子', '若い', 'さようなら', 'とろっとろ', 'ウドウ', '付き合い', 'テンション-tension', '顔', '其処', '芸能', '牛タン', '反対', '神', '手分け', 'そんな', '左右', '締まる', '冴え', '帯', '過ぎる', '空中', '暗記', 'パン-pao', '調べる', '度肝', '缶', '特定', 'メジャー-major', '舌', '愛', '殿', '騒ぎ', '築く', '待ち伏せ', '出演', '欄', 'ボーカル-vocal', '作り出す', 'センス-sense', '柚', '信用', 'ムーブメント-movement', '連絡', '変える', '電源', 'さっき', '旋律', '無事', 'が', '異能', '肩', '程度', '最も', '構う', '男子', '奇麗', '当たる', '夢見', '優秀', 'ナックル', 'ソース-sauce', '追い掛ける', 'モデル-model', '高校', '一切', '死', 'ちょん', '中日', '塞ぐ', '相性', 'な', '場所', '羨ましい', '至る', '有り難い', '無謀', '私', '毎回', 'デ', '在', '状況', '往生', '達', '作業', '手伝う', '曲', '能力', 'フォー-for', '届く', '級', '回復', '良く', '目指す', '遠慮', '亡くなる', '詰む', '数多く', '勝手', '半端', 'も', '人', '大好き', '男の子', 'なんだ', 'クロハネ', 'ダブり', '余る', '向け', '教える', '別れる', '以外', '寄越す', 'です', '思い', '学食', '二', '最近', '世', 'っこ', '丸で', '近い', '決まる', '客', 'いてて', '大', '流行る', '向ける', '保つ', '救急', '奮発', '住み着く', 'ファン-fan（熱狂者）', '用途', 'とも', '馬鹿', 'ちっ', '解答', '栗', '道', '打順', '取れる', '有史', '捲る', '了解', '勝算', '追い込む', '何故', 'タカヒト', '慣れる', '浮遊', '益々', '進学', '否', '代わり', '情報', '其方', '人気', 'ラジャー-roger', '全寮', '映像', '極度', '柄', 'り', '回転', '契約', '具体', '．', 'チャンポン', '見事', '経験', '作曲', 'カレー-curry', '急度', '付く', '解剖', 'スクープ-scoop', 'スーパー-super', 'どっこい', '埋める', '進む', '自覚', '遣る気', 'バナナ-banana', '君-代名詞', '抑', '彼奴', '相手', '其奴', 'ヒロシ', '側', '転入', '礼', '注意', '突き立てる', '協力', '通報', '後', 'てる', '結構', '写真', '狭い', '只管', '着替え', '秘伝', '回る', '静まる', '挽く', '全体', '犯人', '連続', '呼吸', 'ゆっさ', '口寄せ', 'ね', 'ライブ-live', '成功', '生', 'メルアド', '川', '力', '慎重', 'ロー-low', '舞台', '関わる', '状態', '伝わる', '言葉', '数量', '手掛かり', '点', '門', '内', 'ぴたり', '剤', '容易い', 'ファースト-first', 'もう', 'きり', 'ごと', '行成', 'ナックル-knuckle', '阿諛', '入れる', '友達', '字', 'タイプ-type', '凄い', '辛い', '変', '臨む', '幾ら', '隣り', 'ＧＰＳ', '売れる', '寝付く', 'つく', 'りん', '直る', 'スペアリブ-spareribs', '答', '関する', 'うー', 'ツー-two', '旅', 'ユサ', '他言', '様々', '一人', 'ショー-Shaw', '敵う', '助かる', '取り敢えず', '手品', '通り', '旨い', 'やれ', '本人', '好き', '偶に', '冷静', '楽しみ', '時々', 'とっとと', '仕様', '受験', '見える', 'オーバー-over', 'レトルト-retort', 'バット-bat(棒)', '交代', 'とく', 'ナイス-nice', '商品', '賢い', '病院', '貯金', '良い', 'オフ-off', 'メッセージ-message', '懲りる', '狙う', '話す', 'サンド-sandwich', '引っ越し', '個', '院', '三', '対象', '送り', '逆', '彼の', '埃', '御', '解く', '毎日', '恐れ', '癖', '大層', 'デビュー-debut', '以上', '室', '突然', 'なり', '大勢', '示す', '誤解', '誌', 'そう-様態', '今晩', 'タオル-towel', '時', '最早', '石鹸', '完全', 'つう', '独りぼっち', '同行-連れ立つ', '…', 'か', '腹', '広がる', '立てる', 'ステーキ-steak', '打つ', '焜炉', '食べる', 'サイトウ', '型', '瞬間', '感じ', 'む', '噂話', '虹鱒', '頃', 'みいはあ', '妥当', 'そう-伝聞', 'だけ', '遠い', '穴', '中指', '学', '材料', '貰う', '防火', '代々', '似る', '過ぎ', '校', 'どう', '正夢', 'カトウ', 'レビ-Levite', 'よし', '布巾', 'さっぱり', '逃げ出す', '注文', '午後', '判別', '此奴', 'まで', '出す', 'ＰＶ', '迷う', '肉', '寂しい', '駅弁', '及び', '戻す', '親権', '維持', '忍び込む', '可笑しい', '破く', '際', '不可思議', '元', 'ストライク-strike', 'シラヤナギ', '後回し', '誓う', '合わせる', '矛盾', '字幕', 'ピンチ-pinch ', 'じゃ', '其れ', 'がる', '薬', '何者', 'レギュラー-regular', '男勝り', 'ピッチ-pitch', '酷い', 'まあ', '掴む', '丁重', '放棄', '忘れる', '焼き殺す', '分かれる', '持つ', '探る', '兼ねる', 'バーベキュー', '振動', '喜ぶ', 'なんて', '制服', '本日', '身', 'すっ', 'うん', '効果', '素敵', 'ファウル-foul', '行き', '食事', '現われる', '決める', '捕まる', '一', 'えっ', '人物', '欠席', '突入', '殴る', '翌年', '専属', '大量', '間違い', '唸る', '更なる', '御飯', '此方', '問題', 'ナンバ', '晴らす', '全員', '知り合い', '燃える', 'クラス-class', '浴びる', '格好', '精々', '与える', '月曜', 'ミサ', '恵まれる', '風', '多い', '国立', '成績', '持ち帰る', '矢', '承諾', '矢張り', '写る', '昨夜', '色々', 'いたこ', '遊ぶ', '並み', '中等', '連中', '下りる', '鼻血', '降参', '視界', '無駄', '授業', '気配', '練習', 'なり-断定', '目立つ', '明晰', '焼き', '朝御飯', '貴様', '陣', '事件', 'キョウト', '眠り病', '自由', '尽きる', '弁当', 'あっ', '窓', 'ハリウッド-Hollywood', 'くそ', '名乗る', '勝負', 'いしょ', '不審', '口', '声', '助ける', '失礼', '対する', '周り', '消す', '追い払う', '塾生', '発揮', '迎える', '温まる', '本物', 'ヒット-hit', 'ストーカー-stalker', '線路', '店', '合う', 'ユミ', 'スチール-steel', 'させる', '入れ替え', '金', '不思議', '指図', '私有', '恋', '奏でる', '隠す', '媚びる', '日々', '売る', 'っ', '回答', '被る', '鮫', '主導', 'ウイナー-winner', '発想', '全く', '立場', '乗り移る', '下さる', '豪語', 'まじ', 'ちょこちょこ', 'モルモット-marmot', '引っ張る', '生きる', 'ルックス-looks', '全部', '死ぬ', 'ワイルド-wild', '煩い', '深い', 'まあまあ', '平気', '邪', '用意', '隠し味', '生徒', 'ビデオ-video', 'ぇ', '様', '喋る', '危害', '此れ', 'ああ', '為さる', 'たり', '日常', '筈', '秀才', '呪い', '為', 'ぞ', '拍子', '捕まえる', 'シスコン-sister complex', '森', 'ゲーム-game', '展開', '風邪', 'ちんけ', 'オトサカ', '時間', '姉', '君', '役目', '呼ばわり', '向かう', '夢見る', '人目', '人格', '家出', 'スパイス-spice', '去る', '席', '献立', 'ばれる', '晩飯', '投合', 'ロック-lock', '高等', '振り逃げ', '於く', '疲れる', '監視', '分かる', '喧嘩', '予選', '眠気', '＠', '基本', '意識', 'どころ', '犠牲', '転校', 'プチ-petit', 'ヒューマノイド-humanoid', '番組', '安い', '夕食', '見守る', '未来', '巻き込む', 'サキ', 'マドンナ-Madonna', '教室', '健康', '食欲', '恐怖', '居る', '遂に', '探す', 'オーディション-audition', '引き継ぐ', '切れる', '物凄い', '拭く', '朝', '共', 'と', 'ばらす', 'リーダー-leader', '魔', '交換', '電話', 'ハロー-hello', 'オムライス', 'け', 'ヒノモリ', '失う', '要る', '放置', '影響', '美しい', 'あの', '遠ざける', '食費', '期待', '特待', '間隔', '大臣', '最初', '寝坊', '甘み', '脅す', '湯', 'た', '仲直り', 'フレンチ-French', '試合', '末代', 'くる', 'ええ', '則', '遥か', '高い', '腰', '下校', '今朝', '残念', '使い込み', '卒業', '動揺', 'ＣＤ', '週末', '広い', '可笑しな', 'ロッカー-locker', '止める', 'クロバネ', '近所', 'たい', '巡る', '箱', '流石', '下がる', '晩', '名前', '砂糖', '会長', '無用', '変化', '骨折', '素質', '咲き', '渡す', '白', '何れ', 'ショート-short', '二人', '予想', 'ドキュメント-document', '伝える', '被験', 'プレーヤー-player', '限り', 'メート-mate', 'レコード-record', '一昨日', '本名', 'ずっと', '必要', '不安', '俺', '弾く', '煩わしい', '需要', '生かす', '精神', '透明', '以下', '少ない', 'てく', '最', 'など', '特製', '始める', 'めえ', '此の', '通称', '辺り', 'クロハ', 'スパゲッティ-spaghetti', 'サウンド-sound', '電波', '長', '出る', '決して', '園', '無視', '長期', '必死', '悪い', '発火', '恩人', '一々', '大袈裟', '違反', '思い切り', 'めでたい', '素晴らしい', '驚く', '例えば', '見', '拾う', 'ゴロ-grounder', '見渡す', '退', '見せ場', '自業', '行き渡る', '告げる', '迸る', '前提', '合格', '発症', 'クリーム-cream', '者', 'チャンス-chance', '万全', '部', '＆', '普段', '思いの丈', '死者', '唯', '現役', '食器', '参り', '光景', '熱', '｜', 'そろそろ', '崩壊', '小', '為る', 'ファッション-fashion', 'トースト-toast', '勝ち目', 'グランプリ-grand prix', 'コピー-copy', '褒める', 'ライター-writer', '保健', '映る', '登用', 'おお', '流れる', '終わる', '美形', '粥', '跡', '所為', 'トライ', '慈悲', '目的', '塒', '伝統', '確実', '性', '吹く', 'だ', '至当', 'って', '受かる', 'こら', '御前', '大学', '憶測', '付き合う', '一緒', '中', '家計', '芸名', '意味', 'よ', '加わる', '交流', '付き', '首', '仲良く', '由', '撮影', '視認', '飛び下りる', '零', '善哉', '遣る', '挟む', '然', '特別', '論点', '手早い', '一方', '再生', '駆け抜ける', '大事', '振り返し', '診断', '代わり番', '噛む', 'へえ', 'カンナイ', '車', '貧乏', '簡単', '員', '○', '頑張る', 'もてる', '楽園', '独り立ち', 'たって', '手当て', '侭', '試練', '休む', '特上', '任す', '貴方', '見張る', '余計', '遅れる', '明日', '病気', '親', 'もの', '決着', '投げる', '心配', '参る', 'いー', 'コンテスト-contest', '何処', '加減', '呼び出す', 'やばい', '然も', 'ユウ', '間に合う', '警備', '飛ぶ', '教師', '野暮', '押し掛ける', '積り', '仕舞い', '話し合う', '荷物', '釣り合う', '孤独', '甘い', '準備', 'パンケーキ-pancake', 'マシュマロ-marshmallow', '蟹', '良し', 'ローテーション-rotation', '強い', '徐々', 'オオムラ', '夕飯', '景色', '彼是', 'き', '臭い', '弱い', 'まー', '照れ隠し', '度', '着痩せ', '家', 'キオスク-kiosk', '真実', 'きつい', '女性', '勇気', 'はい', '片付く', '気', '運', '壊れる', '移る', '惜しむ', '冷たい', '投手', '破壊', '同士', '肉体', '事実', 'ごろごろ', '検査', '果てる', '詰まり', '不肖', '定石', 'なんか', '玉砕', '待つ', '医者', '類い稀', '白い', '枯れ草', '美味しい', 'ばかり', '退学', '止血', '録画', '賭け', '中学', '秒', '始まる', 'とる', '球種', 'えーと', '即', '売り捌く', '足跡', 'へ', '走る', '不', '来る', 'スマホ', '早い', '寿司', 'ショット-shot', '祝い', '狡猾', 'オイカワ', 'ボール-ball', '研究', '乗せる', '末', '神経', '漸く', 'んー', '才能', '任せる', '封印', 'バンド-band（帯）', 'や', '狡', '腹癒せ', '女子', '走り抜ける', '光', '片付ける', '起きる', '人間', '大男', '行く', '混み合う', 'れる', 'ちょ', 'リカ', '通り抜ける', '動かす', '何の', '和', '似合い', '敷地', '円-助数詞', 'ど', '消える', '社会', '自分', '呪う', '半年', '見当', '逃げる', 'さっ', '防具', '故', '促す', '制御', '来店', 'アクシデント-accident', 'ー', '掛かる', '答案', '同情', 'ブロッコリー-broccoli', '若し', '無', '人差し', '目当て', '焼く', '今後', '煙', '？', '聞き出す', '思い出す', 'ジョー-外国', '今日', 'オカルト-occult', 'テーブル-table', '通う', '実力', '石', '両親', '様子', '志望', 'グラウンド-ground', 'セーフ-safe', '引く-他動詞', 'デメリット-demerit', '振る', 'エンジョイ-enjoy', 'カンニング-cunning', '速攻', '現状', '念', '壊す', '因み', '変装', '揺るぐ', '同じ', '違う', '主', '馳走', '両目', '勿体', '会心', '故人', '先週', 'せる', '病', '時期', '口止め', '突き出す', '見せる', '警告', '式', '実', 'ばら蒔く', '平日', '寝る', 'らしい', '快挙', '死角', 'ちゃん', '悪夢', '癒す', '自炊', '速度', '相応', '減る', '確か', '面白い', '！', '念写', '私-代名詞', '気分', '茶', '特に', '—', 'うんと', '人体', '予約', '事', '相当', 'キン', '賢明', 'げろ', '押さえる', '転機', '非常', '聴取', 'ぶつかる', '判断', '今回', '要', 'マナー-manner', 'ミート-meet', '風呂', 'コヤマ', '世話', 'タイミング-timing', '逮捕', 'マスク-mask', '破廉恥', '勉強', '前兆', '友情', '来客', '直入', '確かめる', 'さ', '集合', '歩く', 'ハロハロ', '術', 'られる', '止め', '面', '苦労', '新入', '一体', '思春', '皆', '視聴', '面会', '味', 'リンチ-lynch', '意気', '痛い', 'ユリコ', '全', '認める', '無くなる', '昼間', '便利', '結果', '嫌々', 'で', 'どうぞ', 'ラジオ-radio', '出来事', '残り', '吐く', '広大', 'し', '黒い', '振り', '患者', 'テレビ-television', '厄介', '土', 'ばい', '球', '潰れる', '鎮静', '結局', '気っ風', '気付く', '疲れ', '文句', '楽しい', '中々', '揺れる', '直様', 'せめて', 'クッション-cushion', '占い', '選ぶ', '揃う', '動く', '第', '聞き込み', '誰', '詰まる', 'いけメン', '離婚', '姿', '路地', '聞こえる', '紹介', 'キジマ', '食う', 'メロディー-melody', '入り', '新聞', '世の中', 'しょっぴく', '説明', '急ぐ', '専門', '紙', '全て', '疑問', '負ける', '素', '小松', '許す', 'メール-mail', '理由', 'ピザ-pizza', '調達', 'のろのろ', '捕球', '我', '直情', '焦れる', '微', 'ゆさり', '騒ぐ', '言い返す', '救う', '伯父', 'さっさと', '移動', '抜群', '更に', '長い', '存分', '泊まり', '今', 'は', '下', '渡る', '分', '昨日', '調子', 'ストレート-straight', '私利', 'はぶる', '辿り着く', '弓道', '見掛け', '音楽', '何時', '内緒', '底', '偏食', '野村', '上昇', '夢中', 'ナオ', '非力', '送る', 'から', '捜査', '出席', '恨み', 'ほど', '見詰める', '十分', '考える', '混乱', '絶対', '危険', '巻く', 'フォーク-fork', '命', '組む', 'ば', '狂う', '挙動', 'つ', 'クラブ-club', 'ごとし', '今更', '入る', '投球', 'アシスト-assist', '笑う', 'やら', '何事', '夢', '警察', '仕事', 'たっぷり', '女', '証明', '最強', '危ない', 'を', '学園', '吊る', '動力', '黙る', '余裕', 'おっと', '仲良し', '何奴', '嬉しい', '空気', 'あんな', '住む', '事情', '自得', '一杯', '御座る', '越える', '同一', 'イメージ-image', '全身', '急', 'ジューシー-juicy', '暇', '限定', '併設', '満更', '駄目', '大人しい', '専用', '予算', '言う', 'ナッシング-nothing', '脳', 'バッテリー-battery', '得意', '接する', '彼女', '申す', '御出座', '量', '超', '面倒', '終える', '最大', '春', '既に', '直ぐ', '困る', '熱い', '大人', '博士', '好物', '満足', 'ベース-base', '開ける', 'コミヤマ', '落とし穴', '麒麟', '才色', '単なる', '計算', '元気', 'ＯＲＧ', '最後', 'ます', '生ずる', '計画', 'クッキー-cookie', '週', '宇', '育てる', '詳しい', '加える', '恐ろしい', '張り', 'おー', 'シングル-single', 'エリート-elite', 'ロック-rock（音楽）', '生活', 'クラスメート-classmate', '次々', 'リン', '新曲', '逝く', '迚も', 'しか', '外', 'カメラ-camera', '溢れる', '掟', '嫌', '申し訳', '使う', '譲る', '証拠', '感激', '対応', 'バンド-band（団）', '特殊', '果たして', '体質', '化け物', 'ギター-guitar', '教科', '真', '落ち着く', '難い', 'スカイ-sky', 'つつ', '落ちる', '通信', '気持ち', '上玉', '唾液', '有る', 'みたい', '物', '胸', '大変', '上げる', '我々-代名詞', 'トモリ', '遅い', '嫌う', 'ほら', '引っ越す', 'ジャック', 'そして', '早退', '暗い', 'トップ-top', '増える', '重ねる', '過ごす', '最高', '感ずる', '数々', '暗示', '添える', '闇', '全然', '何', 'スリル-thrill', '御免', '場面', '心', '星', '撃退', '自慢', '返す', '関係', '実際', '顔色', '有り難う', 'あら', '問い掛ける', '出来る', '業界', '微笑ましい', 'だらけ', '実質', '歩', '発生', 'やがる', '会社', '着込む', '突き動かす', 'スター-star', '殺す', '付属', '仕舞う', 'アウト-out', '名', '目', '皹', '改造', '特効', '初耳', '話', '子', '塾', 'チェック-check', '返る', '母親', '名門', '地面', '因る', '歩む', '正に', '未だ未だ', '落とす', '先程', '部室', '守る', '遠く', '病人', 'もっと', 'に', 'リンチ-Lynch', '所存', '丸焼き', '組', '上', '張る', '狙い', '纏わる', '逆らう', 'よっしゃ', 'おい', '有する', '「', '知る', '玉蜀黍', '恐らく', '寄る', 'サンドイッチ-sandwich', '変わる', '招く', '兄', '権', '抜く', '先', '見付ける', '見る', '学校', 'スポーツ-sport', 'アクセント-accent', '有', '食料', '少し', '脚光', '集める', '爆発', 'ずつ', '何百', '部屋', '校長', '頭', '赤い', 'がち', 'ビンゴ-bingo', '，', 'ださい', '押し付ける', '但し', '出会う', 'バッター-batter', '作戦', '掻き込む', '一瞬', '四', '奴', '所属', '起こる', 'てらっしゃる', '掛ける', '成し遂げる', '後ろ', '初め', '読む', '高', '人生', 'ちゃんと', '奪う', '子供', 'スピリチュアル-spiritual', '断る', '異様', 'シリーズ-series', 'ちまう', '覚える', '路', '黒羽', '日曜', 'うう', 'の', 'ややこしい', '兼備', '間', 'シロップ-siroop', '飲む', '彼処', '電池', '癪', '先ず', '止まる', '丁度', 'あー', '多', '大丈夫', '振るう', 'こそ', '入学', '着信', '呉れる', '乾', 'ぎりぎり', '葬る', '冗談', '細い', '活動', '泣く', '科学', 'プロデューサー-producer', '獲得', '昔', '茶番', '別', '勤める', '叱る', '世界'}\n"
     ]
    }
   ],
   "source": [
    "# Example of the contents of the lemma_database dictionary\n",
    "print(lemma_database['charlotte'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d59775",
   "metadata": {},
   "source": [
    "### Calculate a similarity matrix between each show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0941b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product  # for helping iterate through the shows\n",
    "import numpy as np\n",
    "\n",
    "# Get number of shows\n",
    "num_shows = len(lemma_database)\n",
    "\n",
    "similarity_matrix = np.zeros((num_shows, num_shows))  # zero matrix for over-writing with values\n",
    "\n",
    "# Calculate Jaccard similarity, also create index of show names for future reference\n",
    "def jaccard_similar(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "\n",
    "show_list = []\n",
    "for key in lemma_database:\n",
    "    show_list.append(key)  # add key to show list\n",
    "\n",
    "\n",
    "for i,j in product(range(num_shows), range(num_shows)):\n",
    "    jaccard_value = jaccard_similar(lemma_database[show_list[i]], lemma_database[show_list[j]])\n",
    "    similarity_matrix[i, j] = jaccard_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2de96-c8ba-4de9-a6fe-32231f1a7537",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357301ac-9f8c-438d-b9dc-db9551b7f6b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f643008-74ad-46b5-b682-8595fd5ffefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
